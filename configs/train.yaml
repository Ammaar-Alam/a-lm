# default training hyperparameters for pretraining experiments
optim:
  name: adamw
  lr: 3e-4
  betas: [ 0.9, 0.95 ]
  weight_decay: 0.1
  eps: 1e-8

scheduler:
  name: cosine
  warmup_steps: 50
  max_steps: 2000

training:
  micro_batch_size: 2
  global_batch_size: 128
  seq_len: 512
  eval_interval: 500
  checkpoint_interval: 100
  gradient_clip_norm: 1.0
  gradient_accumulation: 1
  mixed_precision: fp16
  grad_checkpointing: true
  seed: 1337
  max_steps: 2000
  dataloader_workers: 2

logging:
  log_interval: 1
  sample_interval: 2000
  output_dir: runs/pico-pretrain
  rich_progress: true

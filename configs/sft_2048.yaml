optim:
  name: adamw
  lr: 2e-5
  betas: [0.9, 0.95]
  weight_decay: 0.01
  eps: 1e-8

scheduler:
  name: cosine
  warmup_steps: 1000
  max_steps: 30000

training:
  micro_batch_size: 4
  gradient_accumulation: 4
  global_batch_size: 16
  seq_len: 2048
  label_smoothing: 0.0
  checkpoint_interval: 500
  gradient_clip_norm: 0.5
  mixed_precision: bf16
  dataloader_workers: 6
  prefetch_factor: 4
  max_steps: 30000
  seed: 1337

logging:
  log_interval: 50
  rich_progress: true

# Main corpus profile for CUDA training.
# sample_tokens are approximate word counts.
sources:
  fineweb_edu:
    kind: huggingface
    dataset: HuggingFaceFW/fineweb-edu
    split: train
    streaming: true
    sample_tokens: 140000000
    notes: "High-quality web text."

  openwebtext2:
    kind: huggingface
    dataset: Geralt-Targaryen/openwebtext2
    split: train
    streaming: true
    sample_tokens: 120000000
    notes: "General web text."

  wikipedia_en:
    kind: huggingface
    dataset: wikimedia/wikipedia
    config: 20231101.en
    split: train
    streaming: true
    sample_tokens: 100000000
    notes: "Factual reference text."

  pippa_cleaned:
    kind: huggingface
    dataset: royallab/PIPPA-cleaned
    split: train
    streaming: true
    adapter: pippa
    filter_underage: true
    sample_tokens: 260000000
    notes: "Dialogue/mature style corpus."

  # rp_opus:
  #   kind: huggingface
  #   dataset: taozi555/rp-opus
  #   split: train
  #   streaming: true
  #   data_files: messages_remaining.jsonl
  #   adapter: rp_opus
  #   filter_underage: true
  #   sample_tokens: 260000000
  #   notes: "Roleplay/dialog corpus (gated)."

  bluemoon_rp:
    kind: huggingface
    dataset: Squish42/bluemoon-fandom-1-1-rp-cleaned
    split: train
    streaming: true
    filter_underage: true
    sample_tokens: 260000000
    notes: "Roleplay/dialog corpus."

  tinystories:
    kind: huggingface
    dataset: roneneldan/TinyStories
    split: train
    streaming: true
    sample_tokens: 10000000
    notes: "Small regularizer for short coherent generations."

cache:
  base_dir: "${AML_DATA_DIR:-data/cache}"

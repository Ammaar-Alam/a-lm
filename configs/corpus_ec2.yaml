# EC2-focused corpus with larger general web + chat convos
#
# Notes:
# - `sample_tokens` counts whitespace-separated words, not tokenizer tokens.
# - Keep samples conservative to fit within ~100GB disks (clean text + packed shards + checkpoints).
# - rp-opus is a gated dataset; you must have HF access configured (HF_TOKEN / huggingface-cli login).
sources:
  fineweb_edu:
    kind: huggingface
    dataset: HuggingFaceFW/fineweb-edu
    split: train
    streaming: true
    sample_tokens: 200000000
    notes: "High-quality educational web text slice."

  openwebtext2:
    kind: huggingface
    dataset: Geralt-Targaryen/openwebtext2
    split: train
    streaming: true
    sample_tokens: 200000000
    notes: "General web text (OpenWebText2); stream + sample to manage disk."

  wikipedia_en:
    kind: huggingface
    dataset: wikimedia/wikipedia
    config: 20231101.en
    split: train
    streaming: true
    sample_tokens: 150000000
    notes: "English Wikipedia slice."

  pippa_cleaned:
    kind: huggingface
    dataset: royallab/PIPPA-cleaned
    split: train
    streaming: true
    adapter: pippa
    filter_underage: true
    sample_tokens: 50000000
    notes: "Chat logs (may include NSFW)."

  rp_opus:
    kind: huggingface
    dataset: taozi555/rp-opus
    split: train
    streaming: true
    data_files: messages_remaining.jsonl
    adapter: rp_opus
    filter_underage: true
    sample_tokens: 50000000
    notes: "Chat conversations (gated); use messages_remaining.jsonl to avoid <think> traces."

  tinystories:
    kind: huggingface
    dataset: roneneldan/TinyStories
    split: train
    streaming: true
    sample_tokens: 20000000
    notes: "Small amount for early convergence sanity checks; keep low weight."

cache:
  base_dir: "${AML_DATA_DIR:-data/cache}"

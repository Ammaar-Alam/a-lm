model:
  d_model: 384
  n_layers: 8
  n_heads: 6
  n_kv_heads: 1
  ffn_hidden_size: 1536
  vocab_size: 32000
  max_position_embeddings: 1024
  rope_theta: 10000
  alibi: false
  dropout: 0.0
  dual_ffn:
    enabled: false
    router_temperature: 1.0
    capacity_factor: 1.0
    drop_tokens: false
attention:
  type: gqa
  rope_scaling: null
  backend: math

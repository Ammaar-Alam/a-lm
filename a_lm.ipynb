{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a-lm colab training\n",
    "\n",
    "This notebook runs a full from-scratch pretrain on Colab using the larger `nano` config and the Colab corpus preset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional drive mount\n",
    "Use this if your repo or outputs live on Google Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate or clone the repo\n",
    "If you already uploaded the repo, this will use it. Otherwise it clones into `/content/a-lm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "repo_candidates = [\"/content/a-lm\", \"/content/drive/MyDrive/a-lm\"]\n",
    "repo_path = None\n",
    "for candidate in repo_candidates:\n",
    "    if os.path.isdir(candidate):\n",
    "        repo_path = candidate\n",
    "        break\n",
    "\n",
    "if repo_path is None:\n",
    "    repo_path = \"/content/a-lm\"\n",
    "    !git clone https://github.com/ammaar-alam/a-lm.git {repo_path}\n",
    "\n",
    "%cd {repo_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Next: run the 'Install pinned dependencies' cell below.\"\n",
    "    \" Restart only if Colab warns about imports.\"\n",
    "    \" Then continue to 'Hugging Face login' and 'Start pretraining'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pinned dependencies\n",
    "These versions avoid Colab crashes and keep `transformers` compatibility.\n",
    "This cell intentionally does **not** downgrade `numpy` (downgrades force restarts and conflict with Colab preinstalls).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"huggingface_hub<1.0\" \"datasets>=2.19,<3\" \"pyarrow>=15.0.2,<19\" \\\n",
    "  \"fsspec>=2025.3.0\" \"gcsfs>=2025.3.0\" \"tokenizers>=0.22.0,<=0.23.0\"\n",
    "%pip install -e . --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: enable verbose training logs\n",
    "By default the progress bar updates live. If you want per-step log lines, run this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"configs/train_colab_verbose.yaml\").write_text(\"\"\"\n",
    "optim:\n",
    "  name: adamw\n",
    "  lr: 3e-4\n",
    "  betas: [0.9, 0.95]\n",
    "  weight_decay: 0.1\n",
    "  eps: 1e-8\n",
    "\n",
    "scheduler:\n",
    "  name: cosine\n",
    "  warmup_steps: 1000\n",
    "  max_steps: 20000\n",
    "\n",
    "training:\n",
    "  micro_batch_size: 4\n",
    "  gradient_accumulation: 8\n",
    "  max_steps: 20000\n",
    "  checkpoint_interval: 500\n",
    "  gradient_clip_norm: 0.5\n",
    "  mixed_precision: fp16\n",
    "  grad_checkpointing: false\n",
    "  seed: 1337\n",
    "  dataloader_workers: 2\n",
    "\n",
    "logging:\n",
    "  log_interval: 1\n",
    "  rich_progress: false\n",
    "\"\"\")\n",
    "print(\"Wrote configs/train_colab_verbose.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face login\n",
    "Paste your token when prompted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU check\n",
    "Make sure CUDA is available before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "!nvidia-smi\n",
    "print(torch.__version__, torch.cuda.is_available(), torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start pretraining\n",
    "Stores the run id in `LAST_RUN.txt` so later cells can resume or chat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "Path(\"LAST_RUN.txt\").write_text(run_id)\n",
    "print(\"run:\", run_id)\n",
    "\n",
    "train_cfg = \"configs/train_colab.yaml\"\n",
    "if Path(\"configs/train_colab_verbose.yaml\").exists():\n",
    "    train_cfg = \"configs/train_colab_verbose.yaml\"\n",
    "    print(\"using verbose logging config\")\n",
    "\n",
    "cmd = [\"make\", \"colab-pretrain\", f\"RUN={run_id}\", f\"TRAIN_CFG={train_cfg}\"]\n",
    "print(\"command:\", \" \".join(cmd))\n",
    "result = subprocess.run(cmd)\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(f\"make failed with exit code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with the latest checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "run_id = Path(\"LAST_RUN.txt\").read_text().strip()\n",
    "print(\"using run\", run_id)\n",
    "!make chat RUN={run_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLVR post-training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "run_id = Path(\"LAST_RUN.txt\").read_text().strip()\n",
    "print(\"using run\", run_id)\n",
    "\n",
    "!make rlvr-data\n",
    "!make rlvr-train RUN={run_id}\n",
    "!make chat RUN={run_id} CHECKPOINT=runs/{run_id}/rlvr/ckpt-last.pt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "H100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

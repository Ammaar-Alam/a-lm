{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEm1E2aMX66V"
      },
      "source": [
        "# a-lm colab training\n",
        "\n",
        "This notebook runs a full from-scratch pretrain on Colab using the larger `nano` config and the Colab corpus preset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGJWS3ONX66W"
      },
      "source": [
        "## Optional drive mount\n",
        "Use this if your repo or outputs live on Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dfEgMyKX66W",
        "outputId": "367f12d5-7f70-4b4f-fa81-af48aa0a03bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhi1JU_QX66W"
      },
      "source": [
        "## Locate or clone the repo\n",
        "If you already uploaded the repo, this will use it. Otherwise it clones into `/content/a-lm`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aYYvUd7X66W",
        "outputId": "8afea6ac-4ccb-41df-fa00-32138c7891ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/a-lm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Prefer a repo on Google Drive (persistent). If either location already contains\n",
        "# checkpoints, prefer the one with the newest checkpoint.\n",
        "drive_repo = Path(\"/content/drive/MyDrive/a-lm\")\n",
        "local_repo = Path(\"/content/a-lm\")\n",
        "repo_candidates = [drive_repo, local_repo]\n",
        "\n",
        "\n",
        "def newest_run_mtime(repo: Path) -> float:\n",
        "    runs_dir = repo / \"runs\"\n",
        "    artifacts_dir = repo / \"artifacts\"\n",
        "    if not runs_dir.is_dir() or not artifacts_dir.is_dir():\n",
        "        return -1.0\n",
        "    best = -1.0\n",
        "    for run_dir in runs_dir.glob(\"2*\"):\n",
        "        run_id = run_dir.name\n",
        "        ckpt = run_dir / \"pretrain\" / \"ckpt-last.pt\"\n",
        "        tok = artifacts_dir / run_id / \"tokenizer.json\"\n",
        "        if ckpt.exists() and tok.exists():\n",
        "            best = max(best, float(ckpt.stat().st_mtime))\n",
        "    return best\n",
        "\n",
        "\n",
        "repo_path: Path | None = None\n",
        "drive_mtime = newest_run_mtime(drive_repo) if drive_repo.is_dir() else -1.0\n",
        "local_mtime = newest_run_mtime(local_repo) if local_repo.is_dir() else -1.0\n",
        "\n",
        "if max(drive_mtime, local_mtime) >= 0:\n",
        "    repo_path = drive_repo if drive_mtime >= local_mtime else local_repo\n",
        "elif drive_repo.is_dir():\n",
        "    repo_path = drive_repo\n",
        "elif local_repo.is_dir():\n",
        "    repo_path = local_repo\n",
        "else:\n",
        "    # Fresh clone: default to Drive if mounted.\n",
        "    repo_path = drive_repo if drive_repo.parent.is_dir() else local_repo\n",
        "    !git clone https://github.com/Ammaar-Alam/a-lm.git {repo_path}\n",
        "\n",
        "%cd {repo_path}\n",
        "print(\"repo:\", repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOHuhwrfX66W",
        "outputId": "eb64211e-c3ef-485a-e664-a6d2b39bec74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next: run the 'Install pinned dependencies' cell below. Restart only if Colab warns about imports. Then continue to 'Hugging Face login' and 'Start pretraining'.\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"Next: run the 'Install pinned dependencies' cell below.\"\n",
        "    \" Restart only if Colab warns about imports.\"\n",
        "    \" Then continue to 'Hugging Face login' and 'Start pretraining'.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv17Ug74X66W"
      },
      "source": [
        "## Install pinned dependencies\n",
        "These versions avoid Colab crashes and keep `transformers` compatibility.\n",
        "This cell intentionally does **not** downgrade `numpy` (downgrades force restarts and conflict with Colab preinstalls).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f89SZk8gX66X",
        "outputId": "60d9677e-8e9f-46c2-ce6d-b7d8364e6040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub<1.0 in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Collecting datasets<3,>=2.19\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyarrow<19,>=15.0.2 in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.12/dist-packages (2025.3.0)\n",
            "Collecting gcsfs\n",
            "  Downloading gcsfs-2026.1.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets<3,>=2.19) (2.0.2)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<3,>=2.19) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<3,>=2.19) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<3,>=2.19) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets<3,>=2.19) (0.70.16)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub<1.0)\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets<3,>=2.19) (3.13.3)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs) (4.4.2)\n",
            "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gcsfs\n",
            "  Downloading gcsfs-2025.12.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading gcsfs-2025.10.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading gcsfs-2025.9.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading gcsfs-2025.7.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading gcsfs-2025.5.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.5.0.post1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.5.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "INFO: pip is still looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading gcsfs-2025.3.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.3.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.3.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading gcsfs-2025.2.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2024.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading gcsfs-2024.10.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading gcsfs-2024.9.0.post1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading gcsfs-2024.6.1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs) (2.43.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/dist-packages (from gcsfs) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (from gcsfs) (3.8.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3,>=2.19) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3,>=2.19) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3,>=2.19) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3,>=2.19) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3,>=2.19) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3,>=2.19) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3,>=2.19) (1.22.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs) (6.2.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub<1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub<1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub<1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub<1.0) (2026.1.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs) (2.29.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs) (2.8.0)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs) (1.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<3,>=2.19) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<3,>=2.19) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<3,>=2.19) (2025.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage->gcsfs) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage->gcsfs) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage->gcsfs) (1.27.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<3,>=2.19) (1.17.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.3.1)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gcsfs-2024.6.1-py2.py3-none-any.whl (34 kB)\n",
            "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets, gcsfs\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: gcsfs\n",
            "    Found existing installation: gcsfs 2025.3.0\n",
            "    Uninstalling gcsfs-2025.3.0:\n",
            "      Successfully uninstalled gcsfs-2025.3.0\n",
            "Successfully installed datasets-2.21.0 fsspec-2024.6.1 gcsfs-2024.6.1\n",
            "Obtaining file:///content/a-lm\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: a-lm\n",
            "  Building editable for a-lm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for a-lm: filename=a_lm-0.1.0-0.editable-py3-none-any.whl size=12804 sha256=a6cf9c4dee56979078cfe52f5375a69b315e4f840e6310096efd0fd94ccd0a8a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mql9j3fp/wheels/ef/54/1b/8efeaeaddcf19a3d45f22bca6f53da044937f90e80a0172597\n",
            "Successfully built a-lm\n",
            "Installing collected packages: a-lm\n",
            "Successfully installed a-lm-0.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install -U \"huggingface_hub<1.0\" \"datasets>=2.19,<3\" \"pyarrow>=15.0.2,<19\" \\\n",
        "  \"gcsfs\" \"tokenizers>=0.22.0,<=0.23.0\"\n",
        "%pip install -e . --no-deps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTpq2KZCX66X"
      },
      "source": [
        "## Optional: enable verbose training logs\n",
        "By default the progress bar updates live. If you want per-step log lines, run this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTSMLMbDX66X",
        "outputId": "04ed4e9a-a4fc-450f-f2ac-80244d3c6bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote configs/train_colab_verbose.yaml\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "Path(\"configs/train_colab_verbose.yaml\").write_text(\"\"\"\n",
        "optim:\n",
        "  name: adamw\n",
        "  lr: 3e-4\n",
        "  betas: [0.9, 0.95]\n",
        "  weight_decay: 0.1\n",
        "  eps: 1e-8\n",
        "\n",
        "scheduler:\n",
        "  name: cosine\n",
        "  warmup_steps: 1000\n",
        "  max_steps: 20000\n",
        "\n",
        "training:\n",
        "  micro_batch_size: 4\n",
        "  gradient_accumulation: 8\n",
        "  max_steps: 20000\n",
        "  checkpoint_interval: 500\n",
        "  gradient_clip_norm: 0.5\n",
        "  mixed_precision: fp16\n",
        "  grad_checkpointing: false\n",
        "  seed: 1337\n",
        "  dataloader_workers: 2\n",
        "\n",
        "logging:\n",
        "  log_interval: 1\n",
        "  rich_progress: false\n",
        "\"\"\")\n",
        "print(\"Wrote configs/train_colab_verbose.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCpZPfk3X66X"
      },
      "source": [
        "## Hugging Face login\n",
        "Paste your token when prompted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7bzppoHX66X"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsyOHOVX66X"
      },
      "source": [
        "## GPU check\n",
        "Make sure CUDA is available before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQlmAlIzX66X",
        "outputId": "08ca2ca0-c3a4-42ac-a06e-fc6a6a89570e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 29 21:37:00 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:04:00.0 Off |                    0 |\n",
            "| N/A   31C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "2.9.0+cu126 True 12.6\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "!nvidia-smi\n",
        "print(torch.__version__, torch.cuda.is_available(), torch.version.cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAPAD4fsX66X"
      },
      "source": [
        "## Start pretraining\n",
        "This cell **always** sets `LAST_RUN.txt`.\n",
        "\n",
        "- If it finds an existing run (`runs/<RUN>/pretrain/ckpt-last.pt` + `artifacts/<RUN>/tokenizer.json`), it will reuse it and **skip pretraining**.\n",
        "- If it finds a `*.zip` containing those paths, it will import them and **skip pretraining**.\n",
        "- Otherwise it starts a fresh pretrain run.\n",
        "\n",
        "Tip: you can upload a zip via the left sidebar **Files → Upload** (it lands in `/content`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "QNiV26ASX66X",
        "outputId": "5c23cd94-3dd0-4c9a-d6b4-5dc06e5eb7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run: 20260129-213701\n",
            "using verbose logging config\n",
            "command: make colab-pretrain RUN=20260129-213701 TRAIN_CFG=configs/train_colab_verbose.yaml\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "make failed with exit code 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3778558377.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"make failed with exit code {result.returncode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: make failed with exit code 2"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import time\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# If True, always start a fresh pretrain run (ignores existing checkpoints).\n",
        "force_new_pretrain = False\n",
        "\n",
        "\n",
        "def find_usable_runs(repo: Path) -> list[tuple[float, str]]:\n",
        "    runs_dir = repo / \"runs\"\n",
        "    artifacts_dir = repo / \"artifacts\"\n",
        "    if not runs_dir.is_dir() or not artifacts_dir.is_dir():\n",
        "        return []\n",
        "    candidates: list[tuple[float, str]] = []\n",
        "    for run_dir in runs_dir.glob(\"2*\"):\n",
        "        run_id = run_dir.name\n",
        "        ckpt = run_dir / \"pretrain\" / \"ckpt-last.pt\"\n",
        "        tok = artifacts_dir / run_id / \"tokenizer.json\"\n",
        "        if ckpt.exists() and tok.exists():\n",
        "            candidates.append((float(ckpt.stat().st_mtime), run_id))\n",
        "    candidates.sort()\n",
        "    return candidates\n",
        "\n",
        "\n",
        "def choose_repo_with_runs(candidates: list[Path]) -> Path | None:\n",
        "    best: tuple[float, Path] | None = None\n",
        "    for cand in candidates:\n",
        "        if not cand.is_dir():\n",
        "            continue\n",
        "        runs = find_usable_runs(cand)\n",
        "        if not runs:\n",
        "            continue\n",
        "        mtime, _run_id = runs[-1]\n",
        "        if best is None or mtime > best[0]:\n",
        "            best = (mtime, cand)\n",
        "    return best[1] if best else None\n",
        "\n",
        "\n",
        "def zip_run_ids(zip_path: Path) -> list[str]:\n",
        "    ckpt_runs: set[str] = set()\n",
        "    tok_runs: set[str] = set()\n",
        "    with zipfile.ZipFile(zip_path) as zf:\n",
        "        for name in zf.namelist():\n",
        "            parts = Path(name).parts\n",
        "            for i, part in enumerate(parts):\n",
        "                if (\n",
        "                    part == \"runs\"\n",
        "                    and i + 3 < len(parts)\n",
        "                    and parts[i + 2] == \"pretrain\"\n",
        "                    and parts[i + 3] == \"ckpt-last.pt\"\n",
        "                ):\n",
        "                    ckpt_runs.add(parts[i + 1])\n",
        "                if part == \"artifacts\" and i + 2 < len(parts) and parts[i + 2] == \"tokenizer.json\":\n",
        "                    tok_runs.add(parts[i + 1])\n",
        "    return sorted(ckpt_runs & tok_runs)\n",
        "\n",
        "\n",
        "def extract_runs_and_artifacts(zip_path: Path, dest_repo: Path) -> int:\n",
        "    def rel_path(parts: tuple[str, ...]) -> Path | None:\n",
        "        for i, part in enumerate(parts):\n",
        "            if part in {\"runs\", \"artifacts\"}:\n",
        "                return Path(*parts[i:])\n",
        "            if part == \"data\" and i + 1 < len(parts) and parts[i + 1] in {\"packed\", \"sft_packed\", \"sft\"}:\n",
        "                return Path(*parts[i:])\n",
        "        return None\n",
        "\n",
        "    extracted = 0\n",
        "    with zipfile.ZipFile(zip_path) as zf:\n",
        "        for info in zf.infolist():\n",
        "            if info.is_dir():\n",
        "                continue\n",
        "            parts = Path(info.filename).parts\n",
        "            rel = rel_path(parts)\n",
        "            if rel is None:\n",
        "                continue\n",
        "            dest = dest_repo / rel\n",
        "            if dest.exists():\n",
        "                continue\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with zf.open(info) as src, dest.open(\"wb\") as dst:\n",
        "                shutil.copyfileobj(src, dst)\n",
        "            extracted += 1\n",
        "    return extracted\n",
        "\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "\n",
        "# If another repo location already has checkpoints, switch there.\n",
        "alternate_repos = [Path(\"/content/drive/MyDrive/a-lm\"), Path(\"/content/a-lm\")]\n",
        "best_repo = choose_repo_with_runs(alternate_repos)\n",
        "if best_repo is not None and best_repo.resolve() != repo_root.resolve():\n",
        "    print(\"Found existing run artifacts in:\", best_repo)\n",
        "    os.chdir(best_repo)\n",
        "    repo_root = Path.cwd()\n",
        "    print(\"cwd ->\", repo_root)\n",
        "\n",
        "# Reuse an existing run if present (prevents re-pretraining by accident).\n",
        "if not force_new_pretrain:\n",
        "    runs = find_usable_runs(repo_root)\n",
        "    if runs:\n",
        "        _mtime, run_id = runs[-1]\n",
        "        Path(\"LAST_RUN.txt\").write_text(run_id)\n",
        "        print(\"Found existing pretrain checkpoint; skipping pretraining.\")\n",
        "        print(\"run:\", run_id)\n",
        "    else:\n",
        "        # Try importing from a zip (either in /content, Drive, or the repo parent).\n",
        "        search_dirs = [repo_root, repo_root.parent, Path(\"/content\")]\n",
        "        drive_root = Path(\"/content/drive/MyDrive\")\n",
        "        if drive_root.exists():\n",
        "            search_dirs.append(drive_root)\n",
        "        zip_files: list[Path] = []\n",
        "        for d in search_dirs:\n",
        "            if d.is_dir():\n",
        "                zip_files.extend(sorted(d.glob(\"*.zip\")))\n",
        "\n",
        "        best_zip: tuple[int, float, Path] | None = None\n",
        "        for z in zip_files:\n",
        "            try:\n",
        "                run_ids = zip_run_ids(z)\n",
        "            except zipfile.BadZipFile:\n",
        "                continue\n",
        "            if not run_ids:\n",
        "                continue\n",
        "            score = len(run_ids)\n",
        "            mtime = float(z.stat().st_mtime)\n",
        "            if best_zip is None or (score, mtime) > (best_zip[0], best_zip[1]):\n",
        "                best_zip = (score, mtime, z)\n",
        "\n",
        "        if best_zip is None:\n",
        "            try:\n",
        "                from google.colab import files\n",
        "\n",
        "                print(\"No checkpoints found on disk.\")\n",
        "                print(\"Upload a zip that contains runs/<RUN>/pretrain/ckpt-last.pt and artifacts/<RUN>/tokenizer.json\")\n",
        "                uploaded = files.upload()\n",
        "                for name in uploaded:\n",
        "                    z = Path(name)\n",
        "                    if z.suffix.lower() == \".zip\":\n",
        "                        best_zip = (1, float(z.stat().st_mtime), z)\n",
        "                        break\n",
        "            except Exception as exc:\n",
        "                print(\"Zip import unavailable (not running in Colab?):\", exc)\n",
        "\n",
        "        if best_zip is not None:\n",
        "            zip_path = best_zip[2]\n",
        "            print(\"Importing checkpoints from zip:\", zip_path)\n",
        "            extracted = extract_runs_and_artifacts(zip_path, repo_root)\n",
        "            print(\"files extracted:\", extracted)\n",
        "            runs = find_usable_runs(repo_root)\n",
        "            if runs:\n",
        "                _mtime, run_id = runs[-1]\n",
        "                Path(\"LAST_RUN.txt\").write_text(run_id)\n",
        "                print(\"Imported pretrain checkpoint; skipping pretraining.\")\n",
        "                print(\"run:\", run_id)\n",
        "            else:\n",
        "                raise FileNotFoundError(\"Zip import completed but no usable run was found.\")\n",
        "        else:\n",
        "            print(\"No existing checkpoints found. Starting a fresh pretrain run...\")\n",
        "            force_new_pretrain = True\n",
        "\n",
        "if force_new_pretrain:\n",
        "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    Path(\"LAST_RUN.txt\").write_text(run_id)\n",
        "    print(\"run:\", run_id)\n",
        "\n",
        "    train_cfg = \"configs/train_colab.yaml\"\n",
        "    if Path(\"configs/train_colab_verbose.yaml\").exists():\n",
        "        train_cfg = \"configs/train_colab_verbose.yaml\"\n",
        "        print(\"using verbose logging config\")\n",
        "\n",
        "    cmd = [\"make\", \"colab-pretrain\", f\"RUN={run_id}\", f\"TRAIN_CFG={train_cfg}\"]\n",
        "    print(\"command:\", \" \".join(cmd))\n",
        "    result = subprocess.run(cmd)\n",
        "    if result.returncode != 0:\n",
        "        raise RuntimeError(f\"make failed with exit code {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "runs_dir = repo_root / \"runs\"\n",
        "artifacts_dir = repo_root / \"artifacts\"\n",
        "packed_root = repo_root / \"data\" / \"packed\"\n",
        "\n",
        "# Find candidate runs that have the files needed to resume.\n",
        "candidates: list[tuple[float, str]] = []\n",
        "for run_dir in sorted(runs_dir.glob(\"2*\")):\n",
        "    run_id = run_dir.name\n",
        "    ckpt = run_dir / \"pretrain\" / \"ckpt-last.pt\"\n",
        "    tok = artifacts_dir / run_id / \"tokenizer.json\"\n",
        "    packed = packed_root / run_id\n",
        "    if ckpt.exists() and tok.exists() and packed.exists():\n",
        "        candidates.append((float(ckpt.stat().st_mtime), run_id))\n",
        "\n",
        "if not candidates:\n",
        "    raise FileNotFoundError(\n",
        "        \"No resumable runs found (expected runs/<RUN>/pretrain/ckpt-last.pt + artifacts/<RUN>/tokenizer.json + data/packed/<RUN>)\"\n",
        "    )\n",
        "\n",
        "# Newest checkpoint by modification time.\n",
        "candidates.sort()\n",
        "run_id = candidates[-1][1]\n",
        "Path(\"LAST_RUN.txt\").write_text(run_id)\n",
        "print(\"resuming run:\", run_id)\n",
        "\n",
        "ckpt = runs_dir / run_id / \"pretrain\" / \"ckpt-last.pt\"\n",
        "tok = artifacts_dir / run_id / \"tokenizer.json\"\n",
        "packed = packed_root / run_id\n",
        "\n",
        "train_cfg = \"configs/train_colab.yaml\"\n",
        "if Path(\"configs/train_colab_verbose.yaml\").exists():\n",
        "    train_cfg = \"configs/train_colab_verbose.yaml\"\n",
        "\n",
        "cmd = [\n",
        "    \"python3\",\n",
        "    \"scripts/train_pretrain.py\",\n",
        "    \"--model\",\n",
        "    \"configs/nano.yaml\",\n",
        "    \"--train\",\n",
        "    train_cfg,\n",
        "    \"--data\",\n",
        "    str(packed),\n",
        "    \"--out\",\n",
        "    str(runs_dir / run_id / \"pretrain\"),\n",
        "    \"--device\",\n",
        "    \"auto\",\n",
        "    \"--tokenizer\",\n",
        "    str(tok),\n",
        "    \"--resume\",\n",
        "    str(ckpt),\n",
        "]\n",
        "\n",
        "print(\"command:\", \" \".join(cmd))\n",
        "result = subprocess.run(cmd)\n",
        "if result.returncode != 0:\n",
        "    raise RuntimeError(f\"resume failed with exit code {result.returncode}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "Y1DxwvdyxXOM",
        "outputId": "75fb6c35-7554-44b2-ed76-540449f672c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resuming run: 20260129-214515\n",
            "command: python3 scripts/train_pretrain.py --model configs/nano.yaml --train configs/train_colab_verbose.yaml --data data/packed/20260129-214515 --out runs/20260129-214515/pretrain --device auto --tokenizer artifacts/20260129-214515/tokenizer.json --resume runs/20260129-214515/pretrain/ckpt-last.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2881803725.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"command:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"resume failed with exit code {result.returncode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDv7iN2TX66X"
      },
      "source": [
        "## Chat with the latest checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "12PgogMqX66Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8cbf632-4e0b-4ec3-bd6d-83dbce092d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using run 20260129-214515\n",
            "python3 scripts/chat_cli.py \\\n",
            "\t--checkpoint runs/20260129-214515/pretrain/ckpt-last.pt \\\n",
            "\t--tokenizer artifacts/20260129-214515/tokenizer.json \\\n",
            "\t--device auto\n",
            "Loaded model on cuda. Context window ~2048 tokens.\n",
            "Type /exit to quit. Press Ctrl+C to abort.\n",
            "you> hello\n",
            "alm> New research at the University of Virginia by a research group of women who are at the University of Virginia who are at a high school. The research program at the University of Virginia has been the primary school of a career in the field of education. The program is a primary school of high school and is the most common for women in their college and in the school. The program is a program of the program program that is conducted by the school district of the University of Virginia. \n",
            " The first New World is the U.S. National Science and Technology Program (NCI), which is a team of researchers and University researchers. The\n",
            "you> how are you doing?\n",
            "alm> The U.S. National Science Foundation (NSF) is a joint organization of the U.S. Department of Energy's National Energy and Environmental Design Research Institute (NSF), a U.S. Department of Energy and Environmental Economics), the U.S. National Energy and Environmental (NSF), and the U.S. Department of Energy (DOE), the U.S. Environmental Protection Agency. The U.S. Environmental Protection Agency is a a U.S. Department of Energy and a federal agency the U.S. Department of Energy, NIDA, and Environmental Protection Agency (EPA),\n",
            "you> \n",
            "Interrupted.\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def resolve_run_id() -> str:\n",
        "    last = Path(\"LAST_RUN.txt\")\n",
        "    if last.exists():\n",
        "        value = last.read_text().strip()\n",
        "        if value:\n",
        "            return value\n",
        "\n",
        "    candidates: list[tuple[float, str]] = []\n",
        "    for run_dir in sorted(Path(\"runs\").glob(\"2*\")):\n",
        "        run_id = run_dir.name\n",
        "        ckpt = run_dir / \"pretrain\" / \"ckpt-last.pt\"\n",
        "        tok = Path(\"artifacts\") / run_id / \"tokenizer.json\"\n",
        "        if ckpt.exists() and tok.exists():\n",
        "            candidates.append((float(ckpt.stat().st_mtime), run_id))\n",
        "\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No runs found. Run the 'Start pretraining' cell (it can import a zip) first.\"\n",
        "        )\n",
        "\n",
        "    candidates.sort()\n",
        "    run_id = candidates[-1][1]\n",
        "    last.write_text(run_id)\n",
        "    return run_id\n",
        "\n",
        "\n",
        "run_id = resolve_run_id()\n",
        "print(\"using run\", run_id)\n",
        "!make chat RUN={run_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sft-stage"
      },
      "source": [
        "## SFT (instruction tuning)\n",
        "\n",
        "Pretraining teaches general language modeling, but not chat behavior. SFT is the step that teaches the `System/User/Assistant` prompt format used by `scripts/chat_cli.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sft-prepare"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "def resolve_run_id() -> str:\n",
        "    last = Path(\"LAST_RUN.txt\")\n",
        "    if last.exists():\n",
        "        value = last.read_text().strip()\n",
        "        if value:\n",
        "            return value\n",
        "\n",
        "    candidates: list[tuple[float, str]] = []\n",
        "    for run_dir in sorted(Path(\"runs\").glob(\"2*\")):\n",
        "        run_id = run_dir.name\n",
        "        ckpt = run_dir / \"pretrain\" / \"ckpt-last.pt\"\n",
        "        tok = Path(\"artifacts\") / run_id / \"tokenizer.json\"\n",
        "        if ckpt.exists() and tok.exists():\n",
        "            candidates.append((float(ckpt.stat().st_mtime), run_id))\n",
        "\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No runs found. Run the 'Start pretraining' cell (it can import a zip) first.\"\n",
        "        )\n",
        "\n",
        "    candidates.sort()\n",
        "    run_id = candidates[-1][1]\n",
        "    last.write_text(run_id)\n",
        "    return run_id\n",
        "\n",
        "\n",
        "run_id = resolve_run_id()\n",
        "print(\"using run\", run_id)\n",
        "\n",
        "sft_jsonl = Path(f\"data/sft/{run_id}/clean.jsonl\")\n",
        "sft_jsonl.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set an integer for quicker iterations (e.g. 20000). Leave as None to use full datasets.\n",
        "max_per_source = None\n",
        "\n",
        "cmd = [\"python3\", \"scripts/prepare_sft.py\", \"--out\", str(sft_jsonl)]\n",
        "if max_per_source is not None:\n",
        "    cmd += [\"--max-per-source\", str(max_per_source)]\n",
        "print(\"command:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sft-pack"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "def resolve_run_id() -> str:\n",
        "    last = Path(\"LAST_RUN.txt\")\n",
        "    if last.exists():\n",
        "        value = last.read_text().strip()\n",
        "        if value:\n",
        "            return value\n",
        "\n",
        "    candidates: list[tuple[float, str]] = []\n",
        "    for run_dir in sorted(Path(\"runs\").glob(\"2*\")):\n",
        "        run_id = run_dir.name\n",
        "        ckpt = run_dir / \"pretrain\" / \"ckpt-last.pt\"\n",
        "        tok = Path(\"artifacts\") / run_id / \"tokenizer.json\"\n",
        "        if ckpt.exists() and tok.exists():\n",
        "            candidates.append((float(ckpt.stat().st_mtime), run_id))\n",
        "\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No runs found. Run the 'Start pretraining' cell (it can import a zip) first.\"\n",
        "        )\n",
        "\n",
        "    candidates.sort()\n",
        "    run_id = candidates[-1][1]\n",
        "    last.write_text(run_id)\n",
        "    return run_id\n",
        "\n",
        "\n",
        "run_id = resolve_run_id()\n",
        "tok = Path(f\"artifacts/{run_id}/tokenizer.json\")\n",
        "sft_jsonl = Path(f\"data/sft/{run_id}/clean.jsonl\")\n",
        "sft_packed = Path(f\"data/sft_packed/{run_id}\")\n",
        "\n",
        "seq_len = 384\n",
        "shard_size = 1_000_000\n",
        "workers = 6\n",
        "chunk_size = 64\n",
        "\n",
        "cmd = [\n",
        "    \"python3\",\n",
        "    \"scripts/pack_sft.py\",\n",
        "    \"--tokenizer\",\n",
        "    str(tok),\n",
        "    \"--jsonl\",\n",
        "    str(sft_jsonl),\n",
        "    \"--out\",\n",
        "    str(sft_packed),\n",
        "    \"--seq-len\",\n",
        "    str(seq_len),\n",
        "    \"--shard-size\",\n",
        "    str(shard_size),\n",
        "    \"--workers\",\n",
        "    str(workers),\n",
        "    \"--chunk-size\",\n",
        "    str(chunk_size),\n",
        "]\n",
        "print(\"command:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sft-train"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "def resolve_run_id() -> str:\n",
        "    last = Path(\"LAST_RUN.txt\")\n",
        "    if last.exists():\n",
        "        value = last.read_text().strip()\n",
        "        if value:\n",
        "            return value\n",
        "\n",
        "    candidates: list[tuple[float, str]] = []\n",
        "    for run_dir in sorted(Path(\"runs\").glob(\"2*\")):\n",
        "        run_id = run_dir.name\n",
        "        ckpt = run_dir / \"pretrain\" / \"ckpt-last.pt\"\n",
        "        tok = Path(\"artifacts\") / run_id / \"tokenizer.json\"\n",
        "        if ckpt.exists() and tok.exists():\n",
        "            candidates.append((float(ckpt.stat().st_mtime), run_id))\n",
        "\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No runs found. Run the 'Start pretraining' cell (it can import a zip) first.\"\n",
        "        )\n",
        "\n",
        "    candidates.sort()\n",
        "    run_id = candidates[-1][1]\n",
        "    last.write_text(run_id)\n",
        "    return run_id\n",
        "\n",
        "\n",
        "run_id = resolve_run_id()\n",
        "tok = Path(f\"artifacts/{run_id}/tokenizer.json\")\n",
        "init_ckpt = Path(f\"runs/{run_id}/pretrain/ckpt-last.pt\")\n",
        "sft_packed = Path(f\"data/sft_packed/{run_id}\")\n",
        "sft_out = Path(f\"runs/{run_id}/sft\")\n",
        "\n",
        "model_cfg = \"configs/nano.yaml\"\n",
        "train_cfg = \"configs/sft.yaml\"\n",
        "\n",
        "cmd = [\n",
        "    \"python3\",\n",
        "    \"scripts/train_sft.py\",\n",
        "    \"--model\",\n",
        "    model_cfg,\n",
        "    \"--train\",\n",
        "    train_cfg,\n",
        "    \"--data\",\n",
        "    str(sft_packed),\n",
        "    \"--out\",\n",
        "    str(sft_out),\n",
        "    \"--device\",\n",
        "    \"auto\",\n",
        "    \"--init\",\n",
        "    str(init_ckpt),\n",
        "    \"--tokenizer\",\n",
        "    str(tok),\n",
        "]\n",
        "print(\"command:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sft-chat"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def resolve_run_id() -> str:\n",
        "    last = Path(\"LAST_RUN.txt\")\n",
        "    if last.exists():\n",
        "        value = last.read_text().strip()\n",
        "        if value:\n",
        "            return value\n",
        "\n",
        "    candidates: list[tuple[float, str]] = []\n",
        "    for run_dir in sorted(Path(\"runs\").glob(\"2*\")):\n",
        "        run_id = run_dir.name\n",
        "        ckpt = run_dir / \"pretrain\" / \"ckpt-last.pt\"\n",
        "        tok = Path(\"artifacts\") / run_id / \"tokenizer.json\"\n",
        "        if ckpt.exists() and tok.exists():\n",
        "            candidates.append((float(ckpt.stat().st_mtime), run_id))\n",
        "\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No runs found. Run the 'Start pretraining' cell (it can import a zip) first.\"\n",
        "        )\n",
        "\n",
        "    candidates.sort()\n",
        "    run_id = candidates[-1][1]\n",
        "    last.write_text(run_id)\n",
        "    return run_id\n",
        "\n",
        "\n",
        "run_id = resolve_run_id()\n",
        "print(\"using run\", run_id)\n",
        "!make chat RUN={run_id} CHECKPOINT=runs/{run_id}/sft/ckpt-last.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1qMPPsnX66Y"
      },
      "source": [
        "## RLVR post-training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otBTVf_RX66Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b657306-7b9b-475a-e675-9515efa91f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using run 20260129-214515\n",
            "python3 scripts/generate_rlvr_math.py --out data/rlvr/math.jsonl --count 20000\n",
            "Wrote 20,000 examples to data/rlvr/math.jsonl\n",
            "python3 scripts/train_rlvr.py \\\n",
            "\t--init runs/20260129-214515/pretrain/ckpt-last.pt \\\n",
            "\t--tokenizer artifacts/20260129-214515/tokenizer.json \\\n",
            "\t--data data/rlvr/math.jsonl \\\n",
            "\t--out runs/20260129-214515/rlvr \\\n",
            "\t--device auto\n",
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n",
            "step=1/2000 loss=0.0002 pg=0.0002 kl=0.0000 r_mean=0.000 tok/s=81 wall=1s\n",
            "step=25/2000 loss=0.0026 pg=0.0000 kl=0.0026 r_mean=0.000 tok/s=159 wall=35s\n",
            "step=50/2000 loss=0.0105 pg=-0.0001 kl=0.0105 r_mean=0.000 tok/s=172 wall=72s\n",
            "step=75/2000 loss=0.0421 pg=-0.0000 kl=0.0422 r_mean=0.000 tok/s=173 wall=110s\n",
            "step=100/2000 loss=0.0480 pg=0.0000 kl=0.0480 r_mean=0.000 tok/s=165 wall=148s\n",
            "step=125/2000 loss=0.0697 pg=-0.0000 kl=0.0697 r_mean=0.000 tok/s=170 wall=186s\n",
            "step=150/2000 loss=0.0580 pg=0.0000 kl=0.0580 r_mean=0.000 tok/s=170 wall=224s\n",
            "step=175/2000 loss=0.0651 pg=-0.0000 kl=0.0651 r_mean=0.000 tok/s=171 wall=262s\n",
            "step=200/2000 loss=0.0618 pg=-0.0000 kl=0.0618 r_mean=0.000 tok/s=175 wall=300s\n",
            "step=225/2000 loss=0.0697 pg=0.0000 kl=0.0697 r_mean=0.000 tok/s=169 wall=338s\n",
            "step=250/2000 loss=0.0700 pg=-0.0000 kl=0.0700 r_mean=0.000 tok/s=169 wall=375s\n",
            "step=275/2000 loss=0.0631 pg=-0.0000 kl=0.0631 r_mean=0.000 tok/s=169 wall=415s\n",
            "step=300/2000 loss=0.0666 pg=0.0000 kl=0.0666 r_mean=0.000 tok/s=166 wall=452s\n",
            "step=325/2000 loss=0.0701 pg=-0.0000 kl=0.0701 r_mean=0.000 tok/s=172 wall=490s\n",
            "step=350/2000 loss=0.0623 pg=0.0000 kl=0.0623 r_mean=0.000 tok/s=171 wall=528s\n",
            "step=375/2000 loss=0.0704 pg=-0.0000 kl=0.0704 r_mean=0.000 tok/s=171 wall=566s\n",
            "step=400/2000 loss=0.0763 pg=0.0001 kl=0.0762 r_mean=0.000 tok/s=169 wall=604s\n",
            "step=425/2000 loss=0.0714 pg=0.0000 kl=0.0714 r_mean=0.000 tok/s=163 wall=642s\n",
            "step=450/2000 loss=0.0723 pg=-0.0000 kl=0.0723 r_mean=0.000 tok/s=175 wall=679s\n",
            "step=475/2000 loss=0.0709 pg=-0.0000 kl=0.0709 r_mean=0.000 tok/s=169 wall=717s\n",
            "step=500/2000 loss=0.0693 pg=-0.0000 kl=0.0693 r_mean=0.000 tok/s=168 wall=755s\n",
            "step=525/2000 loss=0.0776 pg=-0.0000 kl=0.0776 r_mean=0.000 tok/s=167 wall=794s\n",
            "step=550/2000 loss=0.0787 pg=0.0000 kl=0.0787 r_mean=0.000 tok/s=169 wall=832s\n",
            "step=575/2000 loss=0.0711 pg=0.0000 kl=0.0711 r_mean=0.000 tok/s=167 wall=870s\n",
            "step=600/2000 loss=0.0704 pg=-0.0000 kl=0.0704 r_mean=0.000 tok/s=169 wall=908s\n",
            "step=625/2000 loss=0.0774 pg=-0.0000 kl=0.0774 r_mean=0.000 tok/s=171 wall=946s\n",
            "step=650/2000 loss=0.0730 pg=-0.0000 kl=0.0730 r_mean=0.000 tok/s=171 wall=984s\n",
            "step=675/2000 loss=0.0791 pg=-0.0000 kl=0.0791 r_mean=0.000 tok/s=174 wall=1022s\n",
            "step=700/2000 loss=0.0740 pg=0.0000 kl=0.0740 r_mean=0.000 tok/s=173 wall=1059s\n",
            "step=725/2000 loss=0.0691 pg=0.0000 kl=0.0691 r_mean=0.000 tok/s=170 wall=1097s\n",
            "step=750/2000 loss=0.0740 pg=0.0000 kl=0.0740 r_mean=0.000 tok/s=172 wall=1135s\n",
            "step=775/2000 loss=0.0789 pg=-0.0000 kl=0.0789 r_mean=0.000 tok/s=166 wall=1175s\n",
            "step=800/2000 loss=0.0830 pg=-0.0000 kl=0.0830 r_mean=0.000 tok/s=165 wall=1213s\n",
            "step=825/2000 loss=0.0742 pg=0.0000 kl=0.0742 r_mean=0.000 tok/s=174 wall=1251s\n",
            "step=850/2000 loss=0.0747 pg=-0.0000 kl=0.0747 r_mean=0.000 tok/s=172 wall=1289s\n",
            "step=875/2000 loss=0.0684 pg=0.0000 kl=0.0684 r_mean=0.000 tok/s=159 wall=1327s\n",
            "step=900/2000 loss=0.0772 pg=-0.0000 kl=0.0772 r_mean=0.000 tok/s=168 wall=1365s\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def resolve_run_id() -> str:\n",
        "    last = Path(\"LAST_RUN.txt\")\n",
        "    if last.exists():\n",
        "        value = last.read_text().strip()\n",
        "        if value:\n",
        "            return value\n",
        "\n",
        "    candidates: list[tuple[float, str]] = []\n",
        "    for run_dir in sorted(Path(\"runs\").glob(\"2*\")):\n",
        "        run_id = run_dir.name\n",
        "        ckpt = run_dir / \"pretrain\" / \"ckpt-last.pt\"\n",
        "        tok = Path(\"artifacts\") / run_id / \"tokenizer.json\"\n",
        "        if ckpt.exists() and tok.exists():\n",
        "            candidates.append((float(ckpt.stat().st_mtime), run_id))\n",
        "\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(\n",
        "            \"No runs found. Run the 'Start pretraining' cell (it can import a zip) first.\"\n",
        "        )\n",
        "\n",
        "    candidates.sort()\n",
        "    run_id = candidates[-1][1]\n",
        "    last.write_text(run_id)\n",
        "    return run_id\n",
        "\n",
        "\n",
        "run_id = resolve_run_id()\n",
        "print(\"using run\", run_id)\n",
        "\n",
        "!make rlvr-data\n",
        "!make rlvr-train RUN={run_id} RLVR_INIT=runs/{run_id}/sft/ckpt-last.pt\n",
        "!make chat RUN={run_id} CHECKPOINT=runs/{run_id}/rlvr/ckpt-last.pt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

Metadata-Version: 2.4
Name: a-lm
Version: 0.1.0
Summary: Alam Language Model research stack
Author: a-lm contributors
License: Apache-2.0
Project-URL: homepage, https://github.com/alam/a-lm
Project-URL: repository, https://github.com/alam/a-lm
Project-URL: issues, https://github.com/alam/a-lm/issues
Keywords: language-model,transformer,machine-learning,fastapi
Classifier: Development Status :: 2 - Pre-Alpha
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: MacOS
Classifier: Operating System :: POSIX :: Linux
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: datasets>=2.19
Requires-Dist: fastapi>=0.115
Requires-Dist: numpy>=1.26
Requires-Dist: pyyaml>=6.0
Requires-Dist: pydantic>=2.6
Requires-Dist: rich>=13.7
Requires-Dist: torch>=2.2
Requires-Dist: tqdm>=4.66
Requires-Dist: uvicorn[standard]>=0.30
Provides-Extra: dev
Requires-Dist: mypy>=1.10; extra == "dev"
Requires-Dist: pre-commit>=3.7; extra == "dev"
Requires-Dist: pytest>=8.2; extra == "dev"
Requires-Dist: ruff>=0.4; extra == "dev"
Requires-Dist: types-requests>=2.32; extra == "dev"
Provides-Extra: docs
Requires-Dist: mkdocs-material>=9.5; extra == "docs"
Dynamic: license-file

# a-lm (Alam Language Model)

(Disclaimer: AI-written readme)

`a-lm` is a from-scratch small language model project that builds a custom tokenizer, efficient Transformer backbone, training pipeline, and inference demo suitable for showcasing on a portfolio. The focus is on understanding every piece – from text preprocessing through alignment and deployment – while keeping the stack lean enough to run on an Apple Silicon laptop.

## What gets built
- Custom byte-fallback tokenizer (BPE + optional unigram) trained on curated English corpora.
- Decoder-only Transformer with RMSNorm, SwiGLU, RoPE/ALiBi, grouped attention, and a dual FFN router.
- Data preparation scripts that download, clean, and pack high-quality open datasets (FineWeb-Edu slice, TinyStories, filtered UltraChat, etc.).
- Training loop for pretraining, supervised fine-tuning, and preference tuning (DPO) with checkpoints and sample generation.
- Inference runtime featuring constrained decoding, INT8 weight-only quantization, a FastAPI streaming API, and a lightweight chat widget.

## Roadmap snapshot
Development proceeds milestone by milestone:
1. **Scaffold & tooling** – package layout, CI, configuration. ✅
2. **Tokenizer** – byte normalization, BPE/Unigram trainers, CLI + tests. ✅
3. **Model core** – attention stack, dual FFN gate, Transformer assembly. ✅
4. **Data pipeline** – dataset download/cleaning, token packing. ⏳
5. **Pretraining loop** – optimizer, scheduler, gradient checkpointing, logging.
6. **Alignment** – SFT followed by lightweight DPO.
7. **Inference & quantization** – INT8 modules, constrained decoding, FastAPI server.
8. **Evaluation & demo** – lm-eval harness, streaming widget, documentation polish.

Reference configurations live under `configs/`:
- `configs/corpus.yaml` enumerates dataset sources and cache expectations.
- `configs/pico.yaml` and `configs/train.yaml` capture the initial model and training hyperparameters for a ~29M parameter “pico” run.

Tokenizer and model layers are available today:
- Train the tokenizer via `python scripts/train_tokenizer.py --input data/corpus/*.txt --vocab-size 32000 --out artifacts/tokenizer.json` (BPE default). A Unigram variant is exposed through `alm.tokenizers.train_unigram` for experiments.
- Tokenizer modules live in `src/alm/tokenizers/` with coverage in `tests/tokenizers/`.
- Core Transformer components (RMSNorm, SwiGLU, RoPE/ALiBi, grouped attention, dual FFN, `TransformerModel`) reside in `src/alm/model/` with shape and cache tests in `tests/model/`.

## Dataset notes
Initial data work focuses on high-quality, permissively licensed corpora. FineWeb-Edu (`https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu`) is the primary web slice; TinyStories and filtered chat datasets supplement it. Upcoming scripts will automate downloads via the Hugging Face `datasets` library (you’ll need a Hugging Face account/token) and emit cleaned text under `data/clean/` before packing into contiguous token shards.

## Current status
Planning and foundational code are in place. Tokenizer tooling and the Transformer backbone are implemented with tests; the next milestone will deliver data preparation scripts and sharded token packs so pretraining can begin.

## Contributing / running along
- Target hardware: Apple Silicon (M2) with PyTorch MPS acceleration; CPU fallbacks remain supported for collaborators.
- `Makefile` targets (`make dev`, `make lint`, `make test`, `make check-mps`) streamline setup.
- Keep `TODO_LIST.md` up to date as you progress through milestones; CI (`.github/workflows/ci.yml`) runs lint + tests on pushes and pull requests.
- Never commit secrets – dataset credentials and tokens belong in a local `.env` (ignored by git).

The README will expand with setup instructions, dataset steps, and evaluation results as each milestone lands.

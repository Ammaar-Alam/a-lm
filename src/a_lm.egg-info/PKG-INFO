Metadata-Version: 2.4
Name: a-lm
Version: 0.1.0
Summary: Alam Language Model research stack
Author: a-lm contributors
License: Apache-2.0
Project-URL: homepage, https://github.com/alam/a-lm
Project-URL: repository, https://github.com/alam/a-lm
Project-URL: issues, https://github.com/alam/a-lm/issues
Keywords: language-model,transformer,machine-learning,fastapi
Classifier: Development Status :: 2 - Pre-Alpha
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: MacOS
Classifier: Operating System :: POSIX :: Linux
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: datasets>=2.19
Requires-Dist: fastapi>=0.115
Requires-Dist: numpy>=1.26
Requires-Dist: pyyaml>=6.0
Requires-Dist: pydantic>=2.6
Requires-Dist: rich>=13.7
Requires-Dist: tokenizers>=0.15
Requires-Dist: torch>=2.2
Requires-Dist: tqdm>=4.66
Requires-Dist: uvicorn[standard]>=0.30
Provides-Extra: dev
Requires-Dist: mypy>=1.10; extra == "dev"
Requires-Dist: pre-commit>=3.7; extra == "dev"
Requires-Dist: pytest>=8.2; extra == "dev"
Requires-Dist: ruff>=0.4; extra == "dev"
Requires-Dist: types-requests>=2.32; extra == "dev"
Provides-Extra: docs
Requires-Dist: mkdocs-material>=9.5; extra == "docs"
Dynamic: license-file

# a-lm (Alam Language Model)

(Disclaimer: AI-written README)

> *From-scratch small language model stack, built out of boredom and curiosity ðŸ¦­.*

## Table of Contents
1. [Project Summary](#project-summary)
2. [Prerequisites](#prerequisites)
3. [Environment Setup](#environment-setup)
4. [Essential Commands](#essential-commands)
5. [Dataset Preparation](#dataset-preparation)
6. [Tokenizer Training](#tokenizer-training)
7. [Token Packing](#token-packing)
8. [Pretraining Loop](#pretraining-loop)
9. [Sampling From Checkpoints](#sampling-from-checkpoints)
10. [Supervised Fine-Tuning](#supervised-fine-tuning)
11. [Repository Layout](#repository-layout)
12. [Testing & Linting](#testing--linting)
13. [Troubleshooting](#troubleshooting)
14. [Next Steps](#next-steps)
15. [Colab Guide](#colab-guide)

---

## Project Summary
`a-lm` walks through every layer required to build a compact decoder-only transformer: custom tokenizer, curated dataset pipeline, Apple-friendly training loop, weight-only quantization, and a streaming chat demo (coming in later phases). The code prioritizes clarity and reproducibility so you can showcase it on a resume and continue expanding the stack.

Highlights:
- Byte-fallback tokenizer (BPE + optional Unigram) trained on curated corpora.
- Transformer core with RMSNorm, SwiGLU, RoPE/ALiBi, grouped attention, dual FFN router.
- Packed dataset format for efficient streaming, simple PyTorch training loop with checkpoint/resume.
- Sampling CLI to inspect checkpoints mid-training.

---

## Prerequisites
- **Python** 3.11 or newer (use `pyenv`, `asdf`, or system Python).
- **Apple Silicon** with macOS 13+ recommended (MPS acceleration). CUDA works too.
- **Hugging Face account** with read token to access FineWeb-Edu and related datasets.

Optional but helpful:
- Homebrew packages (`git`, `llvm`, etc.).
- Adequate disk space for datasets/checkpoints (tens of GBs depending on corpora).

---

## Environment Setup
1. Clone the repository.
2. Create and activate a virtualenv:
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   pip install --upgrade pip
   ```
3. Install project + dev dependencies:
   ```bash
   pip install -e .[dev]
   pre-commit install
   ```
4. Log into Hugging Face (once per machine):
   ```bash
   huggingface-cli login
   # or export HF_TOKEN=... in your shell/profile
   ```

---

## Colab Guide
See `docs/colab.md` for a step-by-step Colab notebook setup, including Drive persistence and the `colab-pretrain` target.
If dataset prep crashes with a `PyGILState_Release` error on Colab, upgrade the HF stack with the pinned versions shown in that guide (keeps `transformers` compatibility).

---

## Essential Commands
| Command | Purpose |
|---|---|
| `make dev` | Install dependencies and pre-commit hooks. |
| `make lint` / `ruff check --fix .` | Lint + autofix using Ruff. |
| `make test` / `pytest` | Run the test suite. |
| `ruff format .` | Apply code formatting. |
| `make check-mps` | Print which device (MPS/CUDA/CPU) PyTorch sees. |
| `make fresh-pretrain` | Fresh corpus â†’ tokenizer â†’ shards â†’ pretrain run (timestamped, M2-friendly defaults). |
| `make chat RUN=20260127-120000` | Chat with a run checkpoint (defaults to pretrain `ckpt-last.pt`). |
| `make rlvr-data` | Generate verifiable math prompts for RLVR. |
| `make rlvr-train RUN=20260127-120000` | Run RLVR from a pretrain checkpoint (math verifier). |

> **Always run:** `ruff check --fix .`, `ruff format .`, and `pytest` before committing changes.

---

## Dataset Preparation
1. Ensure Hugging Face login is active (`huggingface-cli login`).
2. Inspect `configs/corpus_m2.yaml` (default for `make fresh-pretrain`) and `configs/corpus.yaml` (bigger run). They reference:
   - FineWeb-Edu (quality web text slice)
   - Wikipedia snapshot (English)
   - TinyStories (tiny synthetic stories)
   - UltraChat/OASST (instruction data for later SFT)
3. Run the prep script to clean and normalize all active sources:
   ```bash
   python scripts/prepare_corpus.py \
     --src configs/corpus_m2.yaml \
     --out data/clean
   ```
   - Output: `data/clean/*.txt` plus metadata JSON for each source.
   - You can comment out sources in the YAML if you want a smaller initial run.

---

## Tokenizer Training
4. Train a tokenizer (default backend: Hugging Face `tokenizers`, Rust):
   ```bash
   python scripts/train_tokenizer.py \
     --input data/clean/*.txt \
     --vocab-size 32000 \
     --out artifacts/tokenizer.json
   ```
   - Produces `artifacts/tokenizer.json` (Hugging Face tokenizer JSON).
   - For quick iteration on large corpora, add `--max-lines 200000` to sample the corpus before training.
   - The pure-Python BPE trainer is available via `--backend python` for tiny toy corpora, but it will not scale to large datasets.

---

## Token Packing
5. Pack the cleaned text into fixed-length token shards:
   ```bash
   python scripts/pack_dataset.py \
     --tokenizer artifacts/tokenizer.json \
     --in data/clean \
     --out data/packed \
     --seq-len 512 \
     --shard-size 2048
   ```
   UPDATED SCRIPT:
   ```bash
    python scripts/pack_dataset.py \
      --tokenizer artifacts/tokenizer.json \
      --in data/clean \
      --out data/packed \
      --seq-len 512 \
      --shard-size 1000000 \
      --workers 6 \
      --chunk-size 1024
    ```
   - `data/packed/` now holds `shard_*.bin` (uint32 tokens) and `metadata.json` describing seq length, shard list, and total tokens.
   - Adjust `--seq-len`/`--shard-size` to suit memory constraints.

---

## Pretraining Loop
6. Launch a pico-scale pretraining run (auto-detects device). This setup keeps runs to roughly an hour on an M2.
   ```bash
   source .venv/bin/activate                     # reuse the prepared virtualenv
   export PYTORCH_MPS_FAST_MATH=1                # enable fast Metal kernels (optional, Apple Silicon)
   python scripts/train_pretrain.py \
     --model configs/pico.yaml \
     --train configs/train.yaml \
     --data data/packed \
     --out runs/pico-pretrain \
      --device auto \
      --tokenizer artifacts/tokenizer.json
   ```
   - Checkpoints land in `runs/pico-pretrain/` (`ckpt-stepXXXXXX.pt`, `ckpt-last.pt`).
   - Press `Ctrl+C` any time; the loop saves both `ckpt-last.pt` and an interrupt-tagged checkpoint before exiting.
   - Resume later with:
     ```bash
    export PYTORCH_MPS_FAST_MATH=1
    python scripts/train_pretrain.py \
      --model configs/pico.yaml \
      --train configs/train.yaml \
      --data data/packed \
      --out runs/pico-pretrain \
      --device auto \
      --resume runs/pico-pretrain/ckpt-last.pt \
      --tokenizer artifacts/tokenizer.json
     ```
   - Want different hyperparameters? Edit `configs/train.yaml` (batch size, accumulation, warmup, scheduler horizon, checkpoint cadence, DataLoader workers).

### Fresh runs (recommended)
If you want to start completely fresh (new corpus snapshot, tokenizer, packed shards, and run dir):
```bash
make dev
make fresh-pretrain
make chat RUN=20260127-120000
```
`make fresh-pretrain` defaults to `configs/corpus_m2.yaml` and `configs/train_m2.yaml`. To use the larger defaults instead:
```bash
make fresh-pretrain CORPUS_CFG=configs/corpus.yaml TRAIN_CFG=configs/train.yaml
```
To start with a larger model, set `MODEL_CFG`:
```bash
make fresh-pretrain MODEL_CFG=configs/nano.yaml
```

### RLVR (verifiable rewards)
After you have a reasonable checkpoint, you can post-train on synthetic verifiable math tasks:
```bash
make rlvr-data
make rlvr-train RUN=20260127-120000
make chat RUN=20260127-120000 CHECKPOINT=runs/20260127-120000/rlvr/ckpt-last.pt
```

### Understanding the Training Progress UI
When `logging.rich_progress` is `true` (default), the loop renders a Rich status panel:

| Segment | Meaning |
| --- | --- |
| **Bar + %** | Completion relative to `training.max_steps`. The coloured bar advances as each step finishes. |
| **Elapsed** (`0:01:25`) | Wall-clock time since the run (or resume) started. |
| **ETA** (`0:09:17`) | Estimated time remaining given the current token throughput. |
| **loss=â€¦** | Exponentially-smoothed training loss over recent steps. On the pico run weâ€™re tracking, the loss started near 6 and is now hovering around **4.0** by step 2.7k; expect it to fall into the low 3s only after a few hundred million tokens. |
| **lr=â€¦** | Current learning rate from the cosine scheduler. Helpful for spotting warmup or cooldown stages. |
| **tok/s=â€¦** | Tokens processed per second (smoothed). Our current pico run on an M2 tops out around **5â€“5.5k tok/s** with the 8Ã—8 batch layout; treat that as a healthy baseline when experimenting. |

Checkpoint events also log in-line (e.g. `Checkpoint saved at step 600`) so you know when it is safe to stop or resume from disk.

### Tuning `configs/train.yaml`

The training CLI reads hyperparameters from `configs/train.yaml`. Adjust these knobs as you scale up:

| Section | Key | Description & when to change |
| --- | --- | --- |
| `optim` | `lr` | AdamW learning rate. Weâ€™re currently training the pico config at **3e-4**; leave it here while loss is steadily trending down. Only consider lowering it if the loss stalls or spikes. |
| | `betas` | Momentum terms for AdamW. Defaults (0.9, 0.95) are stable; adjust only for advanced experiments. |
| | `weight_decay` | L2 regularisation; lower for tiny datasets, higher for large corpora. |
| | `eps` | Numerical stability constant; rarely needs adjustment. |
| `scheduler` | `warmup_steps` | Linear warmup steps before cosine decay. Raise when training longer or with larger batches. |
| | `max_steps` | Total steps that shape the cosine schedule. Match `training.max_steps` when you extend runs. |
| `training` | `micro_batch_size` | Per-step batch size. Increase until you hit memory limits; combine with accumulation for effective batch. |
| | `gradient_accumulation` | Number of micro batches to accumulate before an optimizer step. Increase to simulate a larger global batch on limited memory. |
| | `global_batch_size` | Target global batch (for reference/documentation). Keep it equal to `micro_batch_size * gradient_accumulation` when you change either. |
| | `seq_len` | Token sequence length. Use shorter sequences for debugging or longer ones when you need more context (requires repacking data). |
| | `checkpoint_interval` | Steps between checkpoint saves. Lower to checkpoint more frequently; higher to reduce disk churn. |
| | `gradient_clip_norm` | Norm for gradient clipping. Lower if you see exploding gradients. |
| | `mixed_precision` | Declared precision mode (`fp16`, `bf16`, etc.). The current loop auto-selects FP16 on GPU/MPS; keep this for reference. |
| | `grad_checkpointing` | Toggle PyTorch gradient checkpointing (saves memory at the cost of extra compute). Not yet wired into the loop. |
| | `max_steps` | Hard stop for training. Extend for longer runs. |
| | `dataloader_workers` | Number of background workers for loading batches. Increase if tokens/sec is bottlenecked by CPU. |
| | `seed` | Global random seed for reproducibility. |
| `logging` | `log_interval` | Step cadence for plain-text logging when Rich UI is disabled. |
| | `sample_interval` | Future hook for auto-sampling. Leave as-is for now. |
| | `output_dir` | Default directory for checkpoints/logs. Change when running multiple experiments. |
| | `rich_progress` | Enables/disables the Rich progress bar. Set to `false` for minimal logging environments. |

When you scale to longer runs, update `training.max_steps`, `scheduler.max_steps`, and (optionally) `checkpoint_interval` together to keep the cosine schedule and checkpoint cadence consistent. If you bump `seq_len`, remember to regenerate the packed dataset at the new length.

---

## Supervised Fine-Tuning
1. Prepare instruction/chat conversations (defaults: OpenAssistant OASST1, Ultrachat, Dolly):
   ```bash
   python scripts/prepare_sft.py --out data/sft/clean.jsonl
   ```
   - Use `--include` or `--max-per-source` to curate the mix without editing the script.
2. Pack the conversations into token + loss-mask shards:
   ```bash
   python scripts/pack_sft.py \
     --tokenizer artifacts/tokenizer.json \
     --jsonl data/sft/clean.jsonl \
     --out data/sft_packed \
     --seq-len 384 \
     --shard-size 1000000 \
     --workers 6 \
     --chunk-size 64
   ```
3. Fine-tune from your preferred pretraining checkpoint:
   ```bash
  unset PYTORCH_MPS_FAST_MATH || true
  python scripts/train_sft.py \
    --model configs/pico_sft.yaml \
    --train configs/sft.yaml \
    --data data/sft_packed \
    --out runs/pico-sft \
    --device auto \
    --init runs/pico-pretrain/ckpt-last.pt \
    --tokenizer artifacts/tokenizer.json
  ```
   > Tip: start from a clean output directory (`rm -rf runs/pico-sft`) when switching
   > tokenizers or architectures so you don't accidentally resume incompatible
   > checkpoints.
  ```bash
  unset PYTORCH_MPS_FAST_MATH || true
  python scripts/train_sft.py \
    --model configs/pico_sft.yaml \
    --train configs/sft.yaml \
    --data data/sft_packed \
    --out runs/pico-sft \
    --device auto \
    --resume runs/pico-sft/ckpt-last.pt \
    --tokenizer artifacts/tokenizer.json
   ```

   - Swap `--init` for `--resume runs/pico-sft/ckpt-last.pt` when continuing the same run.
   - The loop masks user tokens, keeps FP32 loss, uses GradScaler on MPS/CUDA, and skips steps with non-finite gradients.
4. Log qualitative changes with the checklist in `docs/sft_eval_prompts.md` so regressions are easy to spot.

---

## Sampling From Checkpoints
7. Inspect generated text mid-training:
   ```bash
   python scripts/sample_text.py \
     --checkpoint runs/pico-pretrain/ckpt-last.pt \
     --tokenizer artifacts/tokenizer.json \
     --prompt "Hello" \
     --max-tokens 50 \
     --temperature 0.8 \
     --top-k 40 \
     --device auto
   ```
   - Supports top-k + temperature sampling; set `--top-k 0` for greedy decoding.
   - Swap prompts to gauge knowledge/fluency as training progresses.

### Interactive Chat CLI
Once you have a checkpoint, launch an interactive conversation loop:

```bash
python scripts/chat_cli.py \
  --checkpoint runs/pico-pretrain/ckpt-last.pt \
  --tokenizer artifacts/tokenizer.json \
  --device auto \
  --temperature 0.7 \
  --top-k 40 \
  --top-p 0.95 \
  --repetition-penalty 1.1
```

```bash
  python scripts/chat_cli.py \
    --checkpoint runs/pico-sft/ckpt-last.pt \
    --tokenizer artifacts/tokenizer.json \
    --device auto \
    --temperature 0.7 \
    --top-k 40 \
    --top-p 0.95 \
    --repetition-penalty 1.1
```

- Type into the `you>` prompt; enter `/exit` or hit `Ctrl+C` when youâ€™re done.
- The script keeps prior turns within the effective context window (~512 tokens with the default packed dataset). To extend chats, repack `data/packed` with a larger `--seq-len` and raise `model.max_position_embeddings` in your config before resuming training.

---

## Repository Layout
```
.
â”œâ”€ configs/          # YAML configs (corpus sources, model sizes, training hyperparams)
â”œâ”€ scripts/          # CLI entrypoints (tokenizer, corpus prep, packing, training, sampling)
â”œâ”€ src/alm/          # Library code: tokenizers, model, data, utils, etc.
â”œâ”€ tests/            # Pytest suites (tokenizer/model/data/training)
â”œâ”€ data/             # Generated cleaned corpora & packed shards (ignored)
â”œâ”€ artifacts/        # Tokenizer artifacts, future checkpoints (ignored)
â”œâ”€ runs/             # Training outputs (ignored)
â”œâ”€ AGENTS.MD         # Agent playbook
â”œâ”€ TODO_LIST.md      # Milestone tracker
â””â”€ README.md         # You are here
```

---

## Testing & Linting
- Run locally before committing or pushing:
  ```bash
  ruff check --fix .
  ruff format .
  pytest
  ```
- CI (GitHub Actions) re-runs the same commands; keep them green by testing locally first.

---

## Troubleshooting
- **Hugging Face auth errors:** re-run `huggingface-cli login`, ensure the token has read access.
- **OOM during training:** lower `micro_batch_size`, increase `gradient_accumulation`, or reduce `--seq-len` when packing.
- **Slow/broken run:** verify `pip install -e .[dev]` succeeded, run `make check-mps` to confirm device.
- **PyTorch aborts with `OMP: Error #179 ... SHM2`:** install `libomp` (`brew install libomp`). The Makefile auto-prepends `/opt/homebrew/opt/libomp/lib` (or `/usr/local/opt/libomp/lib`) to `DYLD_LIBRARY_PATH` when present.
- **Restart training:** delete or move `ckpt-last.pt` if you want a fresh start; otherwise training resumes automatically.
- **Context length:** the pico config trains on 512-token sequences. If you need longer prompts, increase `training.seq_len`, regenerate `data/packed`, and update `model.max_position_embeddings` before resuming.

---

## Next Steps
- Phase 5 (coming soon): supervised fine-tuning on chat data + DPO alignment.
- Phase 6+: inference server (FastAPI), weight-only INT8 quantization, web chat widget.
- Keep `TODO_LIST.md` updated as milestones close; expand this README when new CLIs/configs appear.

---

## Deployment & Scaling Notes
- **Serving on the web:** The roadmap includes a FastAPI inference server (`src/alm/inference/api.py`) and a streaming widget. Host the API alongside your checkpoint, then embed the widget (or a custom frontend) on your site to relay user messages to that endpoint.
- **Export formats:** Current checkpoints stay in PyTorch format. Phase 7 adds weight-only INT8 export paths (GGUF/Core ML) so you can ship compact artifacts for browsers or mobile.
- **Multi-machine training:** The existing trainer is single-node. To accelerate training, mirror the repo and dataset across machines and wire up PyTorch DistributedDataParallel (via `torchrun --nproc_per_node ...`) so optimizer state stays synchronized. Until DDP support lands in-repo, avoid alternating manual runs because AdamW state diverges between hosts.
